# Script to extract the last_hidden_state from ESM-2 models.
# Author: Sara Tolosa AlarcÃ³n

import torch
import esm
import argparse
import os
import time

def load_esm_model(model_name):
    """Load ESM-2 model"""
    models = {
        "esm2_t6_8M_UR50D": esm.pretrained.esm2_t6_8M_UR50D,
        "esm2_t12_35M_UR50D": esm.pretrained.esm2_t12_35M_UR50D,
        "esm2_t30_150M_UR50D": esm.pretrained.esm2_t30_150M_UR50D,
        "esm2_t33_650M_UR50D": esm.pretrained.esm2_t33_650M_UR50D,
        "esm2_t36_3B_UR50D": esm.pretrained.esm2_t36_3B_UR50D,
        "esm2_t48_15B_UR50D": esm.pretrained.esm2_t48_15B_UR50D,
    }
    
    model, alphabet = models[model_name]()
    n_layers = model.num_layers

    return model, alphabet, n_layers

def read_fasta(file_path):
    """ Read a FASTA file and return a dictionary of IDs and sequences."""
    dict_sequences = {}

    with open(file_path, "r") as file:
        current_sequence_name = None
        current_sequence = ""

        for line in file:
            line = line.strip()
            if line.startswith(">"):
                # New sequence header
                if current_sequence_name is not None:
                    dict_sequences[current_sequence_name] = current_sequence
                current_sequence_name = line[1:]
                current_sequence = ""
            else:
                # Append sequence data
                current_sequence += line

        # Add the last sequence
        if current_sequence_name is not None:
            dict_sequences[current_sequence_name] = current_sequence

    return dict_sequences

def extract_embeddings(model, n_layers, batch_converter, device, data=dict, emb_normalized=True, model_name="", output_path="."):
    
    all_last_hidden_states=[]

    # Iterate over the dictionary of sequences
    for id, sequence in data.items():       
        d = [(str(id), sequence)]
        batch_labels, batch_strs, batch_tokens = batch_converter(d)
        batch_tokens = batch_tokens.to(device)

        # Run the model
        with torch.no_grad():
            results = model(batch_tokens, repr_layers=[n_layers], return_contacts=True)

        # Get the last hidden state:
        last_hidden_state_i = results["representations"][n_layers].detach().cpu()

        # Append the hidden states and attentions to the lists
        all_last_hidden_states.append(last_hidden_state_i)

    # Once we obtain all the results, let's concatenate all the results along the batch_size dimension (dim=0):
    last_hidden_state = torch.cat(all_last_hidden_states, dim=0)

    # Normalize it 
    if emb_normalized:
        last_hidden_state = torch.nn.functional.normalize(last_hidden_state, dim=-1)
        
    # Save the hidden states    
    if not os.path.exists(output_path):
        os.makedirs(output_path)
        
    output_file_path = os.path.join(output_path, f"last_hidden_state_{model_name}.pth")
    torch.save(last_hidden_state, output_file_path)



def main(): 
    # 1. Define input and output arguments
    parser = argparse.ArgumentParser(description="This script extracts the embeddings generated by using a ESM-2 model and \
                                    a given list of protein sequences. This information is stored as a tensor.",
                                    formatter_class=argparse.ArgumentDefaultsHelpFormatter)

    parser.add_argument("-i", "--input_path", type=str, help="mutant library input path.")
    parser.add_argument("-m", "--model_name", type=str, help="ESM-2 model name")
    parser.add_argument("-o", "--output_path",type=str, help="output path.",default="./")
    parser.add_argument("-norm", "--normalized",type=bool, default=False)
    parser.add_argument("-v", "--verbose", type=bool, default=False)
    args = vars(parser.parse_args())

    # Validate the arguments
    if args.input_path is None:
        raise ValueError("The input library is required.")
    if args.model_name is None:
        raise ValueError("Model is required.")
    if args.output_path is None:
        raise ValueError("Output path is required.")

    # 2. Load ESM-2 model
    model, alphabet, n_layers = load_esm_model(args.model_name)
    batch_converter = alphabet.get_batch_converter()
    model.eval()  # disables dropout for deterministic results
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    # Load the mutant library
    data = read_fasta(args.input_path)

    # Run now the model
    extract_embeddings(model, n_layers, batch_converter, device, data, args.normalized, args.model_name, args.output_path)

if __name__ == "__main__":
    main()
    print("""\nWork completed!\n""")